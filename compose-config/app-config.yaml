# config.yaml

api_config:
  host: "0.0.0.0"
  port: 5000
  debug: true

logging_config:
  level: "DEBUG"

predictive_model_config:
  url: "https://demo-multimodelserver.apps.cluster-76sbq.76sbq.sandbox1319.opentlc.com/v2/models/demo/infer"
  token: ""
  verify_ssl: true

milvus_config:
  host: "milvus"
  port: "19530"
  collection_name: "netsentinel"
  secure: false

rag_config:
  remote_llm:
    url: "https://granite-8b-code-instruct-maas-apicast-production.apps.prod.rhoai.rh-aiservices-bu.com:443/v1/chat/completions"
    # url: "http://localhost:53950/v1/chat/completions"
    model_name: "granite-8b-code-instruct-128k"
    # model_name: ""
    token: "55ebc201ab5b010dc54974bc36fa1d0a"
    verify_ssl: true
  num_contexts: 3
  nlu_model_path: "models/rasa/nlu-model.gz"

slack_config:
  slack_channel: "#netsentinel"
  slack_bot_token: "xoxb-7834804921362-7828268418118-L0aRQ3CVXKHsjh09De3Oq7nY"
  slack_signing_secret: "8cceb8f82a6a595ce6dca0e689f05e2d"

kafka_config:
  bootstrap: "kafka:29092"
  raw_topic: "raw-traffic-data"
  processed_topic: "processed-traffic-data"

ocp_config:
  kubeconfig_path: "/root/.kube/config"
  auth_method: "kubeconfig"
  prometheus_url: "http://prometheus-server:9090"
